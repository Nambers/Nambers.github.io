<!DOCTYPE html><html lang="EN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>Python使用urllib/urllib3/requests库+beautifulsoup爬取网页 | Eritque arcus's blog</title><meta name="author" content="Eritque arcus"><meta name="copyright" content="Eritque arcus"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="测试环境是python 3.8.1  urlliburllib提供了一系列用于操作URL的功能。urllib的request模块可以非常方便地抓取URL内容，也就是发送一个GET请求到指定的页面，然后返回HTTP的响应。 使用pip下载: 1pip install urllib  例如对百度搜索界"><link rel="shortcut icon" href="/images/avatar.png"><link rel="canonical" href="https://eritque-arcus.tech/2020/01/06/Python%E4%BD%BF%E7%94%A8urllib,urllib3,requests%E5%BA%93+beautifulsoup%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"><meta name="msvalidate.01" content="E0869752BA6C708CA7062BE42E32FF02"><meta name="google-site-verification" content="ueqWwaaGQYLN7uHenp-lhKRcfshkTsWYryPez-V0jPk"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:void 0,translate:void 0,noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:!1},copy:{success:"Copy successfully",error:"Copy error",noSupport:"The browser does not support"},relativeDate:{homepage:!1,post:!1},runtime:"days",dateSuffix:{just:"Just",min:"minutes ago",hour:"hours ago",day:"days ago",month:"months ago"},copyright:void 0,lightbox:"fancybox",Snackbar:void 0,source:{justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js",css:"https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css"}},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!1,percent:{toc:!0,rightside:!1},autoDarkmode:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"Python使用urllib/urllib3/requests库+beautifulsoup爬取网页",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2023-07-24 23:49:31"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,o){if(0===o)return;const n=864e5*o,a={value:t,expiry:(new Date).getTime()+n};localStorage.setItem(e,JSON.stringify(a))},get:function(e){const t=localStorage.getItem(e);if(!t)return;const o=JSON.parse(t);if(!((new Date).getTime()>o.expiry))return o.value;localStorage.removeItem(e)}},e.getScript=e=>new Promise(((t,o)=>{const n=document.createElement("script");n.src=e,n.async=!0,n.onerror=o,n.onload=n.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(n.onload=n.onreadystatechange=null,t())},document.head.appendChild(n)})),e.getCSS=(e,t=!1)=>new Promise(((o,n)=>{const a=document.createElement("link");a.rel="stylesheet",a.href=e,t&&(a.id=t),a.onerror=n,a.onload=a.onreadystatechange=function(){const e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,o())},document.head.appendChild(a)})),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};const t=saveToLocal.get("theme");"dark"===t?activateDarkMode():"light"===t&&activateLightMode();const o=saveToLocal.get("aside-status");void 0!==o&&("hide"===o?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"));/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})(window)</script><meta name="generator" content="Hexo 7.0.0-rc2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/a.jpg" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr class="custom-hr"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Bl0gs</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/Nambers/ctf-writeups/"><i class="fa-fw fa-solid fa-file-pen"></i><span> CTF-wr1teups</span></a></div><div class="menus_item"><a class="site-page" href="/files/s0bt1t1es/"><i class="fa-fw fa-solid fa-closed-captioning"></i><span> Subt1tles</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categ0r1es</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.travellings.cn/go.html"><i class="fa-fw fas fa-subway"></i><span> Travelling</span></a></div><div class="menus_item"><a class="site-page" href="/ctf-cheatsheet/"><i class="fa-fw fas fa-book"></i><span> ⒸⓉⒻ-Ⓒⓗⓔⓐⓣⓢⓗⓔⓔⓣ</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> L1nks</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> Ab0ut</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url('/images/bg.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Eritque arcus's blog"><span class="site-name">Eritque arcus's blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Bl0gs</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/Nambers/ctf-writeups/"><i class="fa-fw fa-solid fa-file-pen"></i><span> CTF-wr1teups</span></a></div><div class="menus_item"><a class="site-page" href="/files/s0bt1t1es/"><i class="fa-fw fa-solid fa-closed-captioning"></i><span> Subt1tles</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categ0r1es</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.travellings.cn/go.html"><i class="fa-fw fas fa-subway"></i><span> Travelling</span></a></div><div class="menus_item"><a class="site-page" href="/ctf-cheatsheet/"><i class="fa-fw fas fa-book"></i><span> ⒸⓉⒻ-Ⓒⓗⓔⓐⓣⓢⓗⓔⓔⓣ</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> L1nks</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> Ab0ut</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Python使用urllib/urllib3/requests库+beautifulsoup爬取网页</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-01-06T17:51:06.000Z" title="Created 2020-01-06 12:51:06">2020-01-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-07-25T03:49:31.995Z" title="Updated 2023-07-24 23:49:31">2023-07-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Web/">Web</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">1.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>7min</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote><p>测试环境是python 3.8.1</p></blockquote><h1 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h1><p>urllib提供了一系列用于操作URL的功能。<br>urllib的request模块可以非常方便地抓取URL内容，也就是发送一个GET请求到指定的页面，然后返回HTTP的响应。</p><p>使用pip下载:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install urllib</span><br></pre></td></tr></table></figure><p>例如对百度搜索界面的抓取(<a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.baidu.com/">www.baidu.com</a>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">	headers = &#123;</span><br><span class="line">            <span class="string">&#x27;Connection&#x27;</span>: <span class="string">&#x27;Keep-Alive&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html, application/xhtml+xml, */*&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept-Encoding&#x27;</span>: <span class="string">&#x27;gzip, deflate&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/6.1 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;www.so.com&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://www.so.com&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">    respond = urllib.request.urlopen(<span class="string">&#x27;http://www.baidu.com&#x27;</span>, headers=headers)</span><br><span class="line">    <span class="built_in">print</span>(respond.read().decode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>参考网站:<br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.liaoxuefeng.com/wiki/1016959663602400/1019223241745024">廖雪峰的官方网站</a></p><h1 id="urllib3"><a href="#urllib3" class="headerlink" title="urllib3"></a>urllib3</h1><p>urllib3是一个功能强大且友好的Python HTTP客户端。大多数Python生态系统已经使用urllib3，您也应该使用。urllib3带来了Python标准库中缺少的许多关键功能：</p><p>线程安全。<br>连接池。<br>客户端SSL &#x2F; TLS验证。<br>使用分段编码上传文件。<br>重试请求和处理HTTP重定向的助手。<br>支持gzip，deflate和brotli编码。<br>HTTP和SOCKS的代理支持。<br>100％的测试覆盖率。<br>urllib3功能强大且易于使用：<br>下载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install urllib3</span><br></pre></td></tr></table></figure><p>例如抓取百度搜索界面(<a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.baidu.com/">www.baidu.com</a>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> urllib3</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    http = urllib3.PoolManager()</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;Connection&#x27;</span>: <span class="string">&#x27;Keep-Alive&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html, application/xhtml+xml, */*&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;Accept-Encoding&#x27;</span>: <span class="string">&#x27;gzip, deflate&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/6.1 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    r=http.request(<span class="string">&#x27;GET&#x27;</span>, <span class="string">&#x27;http://www.baidu.com&#x27;</span>, headers=headers)</span><br><span class="line">    <span class="built_in">print</span>(r.data)</span><br></pre></td></tr></table></figure><h1 id="requests"><a href="#requests" class="headerlink" title="requests"></a>requests</h1><p>我们已经讲解了Python内置的urllib模块和其升级版urllib3，用于访问网络资源。但是，它用起来比较麻烦，而且，缺少很多实用的高级功能。</p><p>更好的方案是使用requests。它是一个Python第三方库，处理URL资源特别方便。<br>下载：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br></pre></td></tr></table></figure><p>注意有 <strong>s</strong> 在&#x3D;&#x3D;requests&#x3D;&#x3D;中</p><p>例如抓取360搜索结果:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    keyword = <span class="string">&quot;123sad&quot;</span></span><br><span class="line">    keyword = <span class="built_in">input</span>(<span class="string">&quot;请输入你想搜索的内容&quot;</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">		<span class="comment">#添加headers防止被最简单的反爬虫阻止，在chrome按F12后点击Network中一个下滑查看</span></span><br><span class="line">        headers = &#123;</span><br><span class="line">            <span class="string">&#x27;Connection&#x27;</span>: <span class="string">&#x27;Keep-Alive&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html, application/xhtml+xml, */*&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Accept-Encoding&#x27;</span>: <span class="string">&#x27;gzip, deflate&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/6.1 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Host&#x27;</span>: <span class="string">&#x27;www.so.com&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Referer&#x27;</span>: <span class="string">&#x27;https://www.so.com&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">        page = <span class="number">1</span></span><br><span class="line">        <span class="comment">#GET参数列表</span></span><br><span class="line">        kv = &#123;<span class="string">&#x27;q&#x27;</span>: keyword, <span class="string">&#x27;ie&#x27;</span>: <span class="string">&#x27;utf-8&#x27;</span>, <span class="string">&#x27;pn&#x27;</span>: page&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># requests.ReadTimeout(60)</span></span><br><span class="line">        r = requests.get(<span class="string">&quot;http://www.so.com/s&quot;</span>, headers=headers,</span><br><span class="line">                         params=kv)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;url:&#x27;</span>+r.request.url)</span><br><span class="line">        r.raise_for_status()</span><br><span class="line">        html = r.text</span><br><span class="line">        <span class="built_in">print</span>(html)</span><br><span class="line">    <span class="keyword">except</span> requests.HTTPError <span class="keyword">as</span> a:</span><br><span class="line">        <span class="built_in">print</span>(a)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;爬取失败&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;失败&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>参考网站：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.liaoxuefeng.com/wiki/1016959663602400/1183249464292448">廖雪峰的官方网站</a></p><h1 id="笔者在爬取时遇到的问题"><a href="#笔者在爬取时遇到的问题" class="headerlink" title="笔者在爬取时遇到的问题"></a>笔者在爬取时遇到的问题</h1><h2 id="1-结果不全"><a href="#1-结果不全" class="headerlink" title="1.结果不全"></a>1.结果不全</h2><p>笔者在vs code 中执行时，结果显示不全<br>如果结果显示不全，需要在cmd中执行文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python xxx.py</span><br></pre></td></tr></table></figure><h2 id="2-‘抓取失败’"><a href="#2-‘抓取失败’" class="headerlink" title="2.‘抓取失败’"></a>2.‘抓取失败’</h2><p>显示该问题，一般是因为IP被屏蔽<br>目前很多搜索引擎都装备了反爬虫，这个时候需要重启路由器（重新拨号）或者挂代理(proxy)，或者试一试别的搜索引擎，笔者抓取百度时频繁出现错误，只能抓取360搜索</p><h2 id="3-返回乱码"><a href="#3-返回乱码" class="headerlink" title="3.返回乱码"></a>3.返回乱码</h2><p>对返回结果解码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">html.decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><h2 id="urllib-1"><a href="#urllib-1" class="headerlink" title="urllib"></a>urllib</h2><h3 id="parse"><a href="#parse" class="headerlink" title="parse"></a>parse</h3><p>使用parse模块拼接参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;<span class="string">&#x27;ie&#x27;</span>: <span class="string">&#x27;utf-8&#x27;</span>, <span class="string">&#x27;wd&#x27;</span>: <span class="string">&#x27;python是这个世界上最好的语言&#x27;</span>&#125;</span><br><span class="line">url = <span class="string">&#x27;www.baidu.com/s?&#x27;</span>+urllib.parse.urlencode(params)</span><br><span class="line"><span class="built_in">print</span>(url)</span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">www.baidu.com/s?ie=utf-8&amp;wd=python%E6%98%AF%E8%BF%99%E4%B8%AA%E4%B8%96%E7%95%8C%E4%B8%8A%E6%9C%80%E5%A5%BD%E7%9A%84%E8%AF%AD%E8%A8%80</span><br></pre></td></tr></table></figure><h3 id="error"><a href="#error" class="headerlink" title="error"></a>error</h3><p>在urllib中设置了两个主要异常类，一个是URLError，一个是HTTPError</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">...</span><br><span class="line"><span class="keyword">except</span> urllib.error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e.code)<span class="comment">#错误码</span></span><br><span class="line">    <span class="built_in">print</span>(e.reason)<span class="comment">#错误的原因</span></span><br><span class="line">    pring(e.headers)<span class="comment">#响应的报头</span></span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)</span><br></pre></td></tr></table></figure><h2 id="re库"><a href="#re库" class="headerlink" title="re库"></a>re库</h2><p>re库是用来分析网页返回结果的正则库<br>安装:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install re</span><br></pre></td></tr></table></figure><p>正则:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">pattern = re.<span class="built_in">compile</span>(</span><br><span class="line">            <span class="string">r&#x27;&lt;li class=&quot;res-list&quot; data-lazyload=&quot;1&quot;&gt;&lt;h3 class=&quot;res-title &quot;&gt;&lt;a href=&quot;(.*?)&quot;&#x27;</span>, re.S)</span><br><span class="line">        results = pattern.findall(html)</span><br></pre></td></tr></table></figure><p>其中,<em>re.compile</em>是创建正则式,<em>findall</em>是在文本中匹配全部，并返回数组格式的数据<br>有关正则表达式写法和更多数据请看<a target="_blank" rel="noopener external nofollow noreferrer" href="https://docs.python.org/zh-cn/3/library/re.html">官方文档</a></p><blockquote><p>笔者推荐使用requests库</p></blockquote><h1 id="beautifulsoup"><a href="#beautifulsoup" class="headerlink" title="beautifulsoup"></a>beautifulsoup</h1><p>Beautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间.<br>pip安装:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install bs4</span><br></pre></td></tr></table></figure><p>如果要使用<code>lxml解析器</code>请参考<a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cnblogs.com/lvdongjie/p/11286599.html">博客园</a>，因为下载太慢，笔者使<code>用html解析器</code></p><p>使用beautifulsoup提取指定的html元素:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">soup = BeautifulSoup(req.text, <span class="string">&quot;html&quot;</span>)</span><br><span class="line"><span class="comment">#req是resquests返回的结果</span></span><br><span class="line">soup.find(name=<span class="string">&quot;div&quot;</span>, attrs=&#123;<span class="string">&quot;class&quot;</span> :<span class="string">&quot;a&quot;</span>,<span class="string">&quot;id&quot;</span>:<span class="string">&quot;b&quot;</span>&#125;)</span><br><span class="line"><span class="comment">#提取&lt;div class=&quot;a&quot; id=&quot;b&quot;&gt;的元素内容</span></span><br><span class="line">soup.select(<span class="string">&quot;.a&quot;</span>)</span><br><span class="line"><span class="comment">#提取class=&quot;a&quot;的元素,可以使用# .等选择器</span></span><br><span class="line">human_list=beautifulsoup(<span class="string">&quot;&lt;p class=&#x27;a&#x27;&gt;&lt;/p&gt;&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(human_list.p[<span class="string">&quot;class&quot;</span>])<span class="comment">#输出 a</span></span><br></pre></td></tr></table></figure><p>如果要取html元素里的内容，用<code>xx.string</code>或者.text<br><a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.jsphp.net/python/show-24-214-1.html">参考链接</a><br><a target="_blank" rel="noopener external nofollow noreferrer" href="https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/">点我跳转官方文档</a></p><h1 id="例子"><a href="#例子" class="headerlink" title="例子:"></a>例子:</h1><p>以下代码爬取了<code>https://www.baidu.com/s?ie=UTF-8&amp;wd=afs</code>的第一条搜索结果的名字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://www.baidu.com/s?ie=UTF-8&amp;wd=afs&quot;</span></span><br><span class="line">headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.87 Safari/537.36&#x27;</span>&#125;</span><br><span class="line">req = requests.get(url, headers=headers)</span><br><span class="line"><span class="comment">#soup = BeautifulSoup(req.text, &quot;lxml&quot;)#使用lxml解析器</span></span><br><span class="line">soup = BeautifulSoup(req.text, <span class="string">&quot;html&quot;</span>)<span class="comment">#使用html解析器</span></span><br><span class="line">human_list=soup.find(name=<span class="string">&quot;div&quot;</span>, attrs=&#123;<span class="string">&quot;class&quot;</span> :<span class="string">&quot;c-abstract&quot;</span>&#125;)</span><br><span class="line">human_list=<span class="built_in">str</span>(human_list)</span><br><span class="line">human_list=human_list.replace(<span class="string">&#x27;&lt;div class=&quot;c-abstract c-abstract-en&quot;&gt;&#x27;</span>,<span class="string">&quot;&quot;</span>)</span><br><span class="line">human_list=human_list.replace(<span class="string">&quot;&lt;/div&gt;&quot;</span>,<span class="string">&quot;&quot;</span>)</span><br><span class="line">human_list=human_list.replace(<span class="string">&quot;\n&quot;</span>,<span class="string">&quot;&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(human_list)</span><br></pre></td></tr></table></figure><p>输出:</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">em</span>&gt;</span>AFS<span class="tag">&lt;/<span class="name">em</span>&gt;</span> study abroad, education and volunteer programs empower people of all ages and backgrounds with essential intercultural knowledge, skills and understanding.</span><br></pre></td></tr></table></figure><center><b>END</b></center></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://eritque-arcus.tech">Eritque arcus</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://eritque-arcus.tech/2020/01/06/Python%E4%BD%BF%E7%94%A8urllib,urllib3,requests%E5%BA%93+beautifulsoup%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5/">https://eritque-arcus.tech/2020/01/06/Python%E4%BD%BF%E7%94%A8urllib,urllib3,requests%E5%BA%93+beautifulsoup%E7%88%AC%E5%8F%96%E7%BD%91%E9%A1%B5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener external nofollow noreferrer" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Python/">Python</a><a class="post-meta__tags" href="/tags/Web-crawler/">Web crawler</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/2020/01/26/%E4%BD%BF%E7%94%A8html+css+js%E5%88%B6%E4%BD%9C%E7%BD%91%E7%AB%99%E6%95%99%E7%A8%8B/" title="使用html+css+js制作登录网站教程"><div class="cover" style="background:var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">使用html+css+js制作登录网站教程</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2021/01/11/%E7%94%A8python%E5%81%9Ayoutube%E8%87%AA%E5%8A%A8%E5%8C%96%E4%B8%8B%E8%BD%BD%E5%99%A8/" title="python自动化下载youtude视频"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-11</div><div class="title">python自动化下载youtude视频</div></div></a></div><div><a href="/2021/03/31/python%E4%B8%80%E9%94%AE%E4%B8%8B%E8%BD%BD%E4%B8%8E%E6%9B%BF%E6%8D%A2hexo%E5%8D%9A%E5%AE%A2%E9%87%8C%E7%9A%84%E5%9B%BE%E7%89%87%E5%9C%B0%E5%9D%80/" title="python一键下载与替换hexo博客里的图片地址"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-31</div><div class="title">python一键下载与替换hexo博客里的图片地址</div></div></a></div><div><a href="/2020/01/26/%E4%BD%BF%E7%94%A8html+css+js%E5%88%B6%E4%BD%9C%E7%BD%91%E7%AB%99%E6%95%99%E7%A8%8B/" title="使用html+css+js制作登录网站教程"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-01-26</div><div class="title">使用html+css+js制作登录网站教程</div></div></a></div><div><a href="/2020/12/13/%E7%94%A8python-sklearn-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%AE%9E%E7%8E%B0%E5%A4%A9%E6%B0%94%E9%A2%84%E6%8A%A5/" title="用python+sklearn(机器学习)实现天气预测"><div class="cover" style="background:var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-13</div><div class="title">用python+sklearn(机器学习)实现天气预测</div></div></a></div></div></div><hr class="custom-hr"><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/a.jpg" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">Eritque arcus</div><div class="author-info__description">Eritque arcus's blog</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/Nambers"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Nambers" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:eritque-arcus@eritque-arcus.tech" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#urllib"><span class="toc-number">1.</span> <span class="toc-text">urllib</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#urllib3"><span class="toc-number">2.</span> <span class="toc-text">urllib3</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#requests"><span class="toc-number">3.</span> <span class="toc-text">requests</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%94%E8%80%85%E5%9C%A8%E7%88%AC%E5%8F%96%E6%97%B6%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">4.</span> <span class="toc-text">笔者在爬取时遇到的问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%BB%93%E6%9E%9C%E4%B8%8D%E5%85%A8"><span class="toc-number">4.1.</span> <span class="toc-text">1.结果不全</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E2%80%98%E6%8A%93%E5%8F%96%E5%A4%B1%E8%B4%A5%E2%80%99"><span class="toc-number">4.2.</span> <span class="toc-text">2.‘抓取失败’</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%BF%94%E5%9B%9E%E4%B9%B1%E7%A0%81"><span class="toc-number">4.3.</span> <span class="toc-text">3.返回乱码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%BF%9B%E9%98%B6"><span class="toc-number">5.</span> <span class="toc-text">进阶</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#urllib-1"><span class="toc-number">5.1.</span> <span class="toc-text">urllib</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#parse"><span class="toc-number">5.1.1.</span> <span class="toc-text">parse</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#error"><span class="toc-number">5.1.2.</span> <span class="toc-text">error</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#re%E5%BA%93"><span class="toc-number">5.2.</span> <span class="toc-text">re库</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#beautifulsoup"><span class="toc-number">6.</span> <span class="toc-text">beautifulsoup</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">7.</span> <span class="toc-text">例子:</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/09/01/john-the-ripper-%E8%A7%A3%E5%AF%86-salted-SHA512/" title="john the ripper 解密加盐后密文">john the ripper 解密加盐后密文</a><time datetime="2023-09-01T22:31:29.000Z" title="Created 2023-09-01 18:31:29">2023-09-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/07/25/Arch-%E5%AE%89%E8%A3%85-Yafu2/" title="Arch Yafu2 安装指南">Arch Yafu2 安装指南</a><time datetime="2023-07-25T22:13:34.000Z" title="Created 2023-07-25 18:13:34">2023-07-25</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/02/10/emacs-configuration/" title="wsl2 / ubuntu 安装和配置 Emacs + mit-scheme">wsl2 / ubuntu 安装和配置 Emacs + mit-scheme</a><time datetime="2023-02-10T23:25:24.000Z" title="Created 2023-02-10 18:25:24">2023-02-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/01/10/eclipse-UTF16.isSurrogate-NoSuchMethodError/" title="Eclipse java.lang.NoSuchMethodError: 'boolean com.ibm.icu.text.UTF16.isSurrogate(char)'">Eclipse java.lang.NoSuchMethodError: 'boolean com.ibm.icu.text.UTF16.isSurrogate(char)'</a><time datetime="2023-01-10T20:06:42.000Z" title="Created 2023-01-10 15:06:42">2023-01-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/12/21/MessageIOException_Could_not_write_127.0.0.1/" title="org.gradle.internal.remote.internal.MessageIOException: Could not write '/127.0.0.1:&lt;random port&gt;'">org.gradle.internal.remote.internal.MessageIOException: Could not write '/127.0.0.1:&lt;random port&gt;'</a><time datetime="2022-12-21T22:24:45.000Z" title="Created 2022-12-21 17:24:45">2022-12-21</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Eritque arcus</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a href="https://icp.gov.moe/?keyword=20222242" target="_blank">萌ICP备20222242号</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn(){"object"==typeof pangu?pangu.autoSpacingPage():getScript("https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js").then((()=>{pangu.autoSpacingPage()}))}function panguInit(){GLOBAL_CONFIG_SITE.isPost&&panguFn()}document.addEventListener("DOMContentLoaded",panguInit)</script><div class="js-pjax"><script>function loadGitalk(){function t(){new Gitalk(Object.assign({clientID:"3b2ee3ad890fd2131443",clientSecret:"daff063416b62436085108e28e22859a7cb598b4",repo:"Nambers.github.io",owner:"Nambers",admin:["Nambers"],id:"dd2a902b3042be3cc98a1b5dd3491e5a",updateCountCallback:commentCount},null)).render("gitalk-container")}"function"==typeof Gitalk?t():(getCSS("https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"),getScript("https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.js").then(t))}function commentCount(t){let e=document.querySelector("#post-meta .gitalk-comment-count");e&&(e.textContent=t)}{function loadOtherComment(){loadGitalk()}btf.loadComment(document.getElementById("gitalk-container"),loadGitalk)}</script></div></div></body></html>